# SEC Financial Data Pipeline â€” Modern ETL Platform with Advanced dbt Transformations

A comprehensive SEC EDGAR financial data extraction and analytics platform that orchestrates the complete lifecycle of financial statement data ingestion, processing, and transformation. This production-ready system demonstrates modern data engineering practices with Snowflake as the core data warehouse, sophisticated dbt transformations, and automated pipeline orchestration.

## ðŸŽ¯ Project Overview

This system addresses the challenge of building a comprehensive master financial statement database to enable better and faster fundamental analysis of US public companies. The platform extracts, transforms, and loads SEC EDGAR financial data through three sophisticated storage approaches: raw staging, JSON transformation, and dimensional modeling.

### Core Business Problem
- **Challenge**: Fragmented SEC financial data requiring complex processing for analysis
- **Solution**: Automated ETL pipeline with multiple data modeling approaches
- **Value**: Streamlined fundamental analysis for financial professionals

## ðŸ—ï¸ System Architecture

### High-Level Data Flow
```
SEC EDGAR Portal â†’ FireCrawl API â†’ Airflow ETL â†’ S3 Storage â†’ Snowflake â†’ dbt Transformations â†’ Analytics Layer
```

### Architecture Components

#### 1. **Data Extraction Layer**
- **Data Source**: SEC Markets Data Portal (`https://www.sec.gov/data-research/sec-markets-data/financial-statement-data-sets`)
- **Method**: FireCrawl API for intelligent web scraping
- **Target Files**: ZIP archives containing TSV files (SUB, NUM, PRE, TAG)
- **Frequency**: Quarterly data extraction with historical backfill capability

#### 2. **Orchestration Layer**
- **Platform**: Apache Airflow with CeleryExecutor
- **Docker Deployment**: Multi-service container orchestration
- **Task Management**: Dynamic parameter-driven DAG execution
- **Monitoring**: Comprehensive logging and health checks

#### 3. **Storage Layer**
- **Staging**: AWS S3 for intermediate file storage
- **Warehouse**: Snowflake with external stage integration
- **Data Lake**: Structured directory organization by year/quarter

#### 4. **Transformation Layer (dbt)**
- **Raw Layer**: Direct TSV to SQL table mapping
- **JSON Layer**: Document-based storage for flexible querying
- **Dimensional Layer**: Star schema with fact and dimension tables

#### 5. **Application Layer**
- **API**: FastAPI for programmatic data access
- **UI**: Streamlit for interactive analytics
- **Integration**: Direct Snowflake connectivity

## ðŸ“ Project Structure

```
sec-financial-data-pipeline/
â”œâ”€â”€ airflow/                          # Orchestration Layer
â”‚   â”œâ”€â”€ docker-compose.yaml          # Multi-service deployment
â”‚   â”œâ”€â”€ config/airflow.cfg           # Airflow configuration
â”‚   â””â”€â”€ dags/                        # Pipeline definitions
â”‚       â”œâ”€â”€ dbt_raw_dag.py          # Raw data processing pipeline
â”‚       â”œâ”€â”€ json_dag.py             # JSON transformation pipeline
â”‚       â”œâ”€â”€ denormalize_data_dag.py # Dimensional modeling pipeline
â”‚       â”œâ”€â”€ get_datalinks.py        # Data discovery pipeline
â”‚       â”œâ”€â”€ scrape_upload_dag.py    # Data extraction pipeline
â”‚       â””â”€â”€ utils/                   # Shared utilities
â”‚           â”œâ”€â”€ main.py             # Core pipeline functions
â”‚           â”œâ”€â”€ aws/s3.py           # S3 integration utilities
â”‚           â””â”€â”€ snowflake/          # Snowflake connectors
â”œâ”€â”€ dbt_sec_cloud-main/             # ðŸŽ¯ dbt Transformation Layer
â”‚   â”œâ”€â”€ dbt_project.yml             # dbt project configuration
â”‚   â”œâ”€â”€ models/                     # SQL transformation models
â”‚   â”‚   â”œâ”€â”€ raw/                    # Stage 1: Raw data ingestion
â”‚   â”‚   â”‚   â”œâ”€â”€ sub.sql            # Submission data model
â”‚   â”‚   â”‚   â”œâ”€â”€ num.sql            # Numeric data model  
â”‚   â”‚   â”‚   â”œâ”€â”€ pre.sql            # Presentation linkage model
â”‚   â”‚   â”‚   â””â”€â”€ tag.sql            # Taxonomy model
â”‚   â”‚   â”œâ”€â”€ json/                   # Stage 2: JSON denormalization
â”‚   â”‚   â”‚   â”œâ”€â”€ json_sub.sql       # Document-based submissions
â”‚   â”‚   â”‚   â”œâ”€â”€ json_num.sql       # Document-based numerics
â”‚   â”‚   â”‚   â”œâ”€â”€ json_pre.sql       # Document-based presentations
â”‚   â”‚   â”‚   â””â”€â”€ json_tag.sql       # Document-based taxonomy
â”‚   â”‚   â””â”€â”€ dw/                     # Stage 3: Dimensional warehouse
â”‚   â”‚       â”œâ”€â”€ DIM_COMPANY.sql    # Company dimension
â”‚   â”‚       â”œâ”€â”€ DIM_TAG.sql        # Financial metric dimension
â”‚   â”‚       â”œâ”€â”€ FACT_BALANCE_SHEET.sql    # Balance sheet facts
â”‚   â”‚       â”œâ”€â”€ FACT_INCOME_STATEMENT.sql # Income statement facts
â”‚   â”‚       â””â”€â”€ FACT_CASH_FLOW.sql        # Cash flow facts
â”‚   â”œâ”€â”€ macros/                     # Reusable SQL macros
â”‚   â”œâ”€â”€ tests/                      # Data quality tests
â”‚   â””â”€â”€ analyses/                   # Ad-hoc analysis queries
â”œâ”€â”€ backend/                        # API Layer
â”‚   â””â”€â”€ fastApi/                    # FastAPI application
â”‚       â”œâ”€â”€ main.py                 # API endpoints and logic
â”‚       â”œâ”€â”€ snowflake_python/       # Database connectors
â”‚       â””â”€â”€ aws/                    # AWS integrations
â”œâ”€â”€ frontend/                       # User Interface
â”‚   â””â”€â”€ streamlit-app.py           # Interactive analytics dashboard
â””â”€â”€ .devcontainer/                 # Development environment
    â””â”€â”€ devcontainer.json          # VS Code container configuration
```

## ðŸ”„ Advanced dbt Transformation Architecture

### dbt Project Configuration

The `dbt_sec_cloud-main/` directory contains a sophisticated multi-layered transformation approach:

```yaml
# dbt_project.yml
name: 'analytics'
models:
  analytics:
    raw:
      +materialized: incremental      # Efficient processing of new data
      +schema: RAW
    json:
      +materialized: incremental      # Document-based storage
      +schema: JSON
      +on_schema_change: "append_new_columns"
    dw:
      +materialized: incremental      # Dimensional warehouse
      +schema: DW
      +on_schema_change: "append_new_columns"
```

### Layer 1: Raw Data Models (`models/raw/`)

#### **`sub.sql` - Company Submission Data**
```sql
-- Dynamic quarterly data loading with metadata filtering
{% set year = var('year', "2024") | string %}
{% set qtr = var('qtr', "4") | string %}
{% set pattern = '%' + year + '/' + qtr + '/sub.tsv' %}

SELECT
    value:c1::STRING AS adsh,           -- EDGAR Accession Number
    TRY_CAST(value:c2::STRING AS INT) AS cik,    -- Central Index Key
    value:c3::STRING AS name,           -- Company Name
    TRY_CAST(value:c4::STRING AS INT) AS sic,    -- Standard Industry Code
    -- Address and filing metadata (35+ fields)
FROM {{ source('stage_source', 'sec_ext_table') }}
WHERE metadata$filename LIKE '{{ pattern }}'
```

**Key Features**:
- **Dynamic Filtering**: Uses dbt variables for year/quarter selection
- **Data Type Casting**: Intelligent type conversion with TRY_CAST
- **Metadata Extraction**: Leverages Snowflake's METADATA$ functions
- **Incremental Processing**: Truncate and reload pattern for data consistency

#### **`num.sql` - Financial Numeric Data**
```sql
-- Financial metrics with comprehensive data validation
SELECT 
    GET(VALUE, 'c1')::STRING AS adsh,         -- Links to submission
    GET(VALUE, 'c2')::STRING AS tag,          -- Financial statement tag
    GET(VALUE, 'c3')::STRING AS version,      -- XBRL taxonomy version
    TRY_TO_DATE(GET(VALUE, 'c4')::STRING, 'YYYYMMDD') AS ddate,
    TRY_CAST(GET(VALUE, 'c5')::STRING AS INT) AS qtrs,    -- Duration in quarters
    TRY_CAST(GET(VALUE, 'c9')::STRING AS INT) AS value    -- Financial value
FROM {{source('stage_source', 'sec_ext_table')}}
WHERE METADATA$FILENAME LIKE '{{pattern}}'
```

**Advanced Features**:
- **Financial Metrics**: Extracts standardized XBRL tags
- **Temporal Handling**: Converts date strings to proper DATE types
- **Value Parsing**: Handles large financial numbers with proper casting

#### **`pre.sql` - Presentation Linkage**
```sql
-- Links financial data to specific statement presentations
SELECT 
    VALUE:c1::STRING AS adsh,               -- Accession number
    TRY_CAST(VALUE:c2::STRING AS INT) AS report,     -- Report identifier
    TRY_CAST(VALUE:c3::STRING AS INT) AS line,       -- Line number
    VALUE:c4::STRING AS stmt,               -- Statement type (BS, IS, CF)
    VALUE:c7::STRING AS tag,                -- Financial tag reference
FROM {{source('stage_source', 'sec_ext_table')}}
WHERE METADATA$FILENAME LIKE '{{pattern}}'
```

### Layer 2: JSON Models (`models/json/`)

The JSON layer implements document-based storage for enhanced query flexibility:

#### **`json_sub.sql` - Document Transformation Pattern**
```sql
{{ config(
    alias='SUB',
    materialized='incremental',
    unique_key='DATA_HASH'
) }}

WITH staged_data AS (
    SELECT 
        OBJECT_CONSTRUCT(*) AS DATA,                    -- Convert row to JSON object
        SHA2(TO_JSON(OBJECT_CONSTRUCT(*)), 256) AS DATA_HASH,  -- Generate unique hash
        CURRENT_TIMESTAMP AS CREATED_DT,
        CURRENT_USER() AS CREATED_BY
    FROM {{source('raw_source', 'sub')}}
)
SELECT *
FROM staged_data
WHERE DATA_HASH NOT IN (SELECT DATA_HASH FROM JSON.SUB)  -- Incremental loading
```

**JSON Layer Benefits**:
- **Schema Flexibility**: Dynamic schema evolution without DDL changes
- **Query Performance**: Optimized for nested data queries
- **Deduplication**: Hash-based incremental processing
- **Audit Trail**: Built-in metadata tracking

### Layer 3: Dimensional Warehouse (`models/dw/`)

#### **Company Dimension (`DIM_COMPANY.sql`)**
```sql
{{ config(
    unique_key=['CIK', 'NAME']
) }}

WITH deduplicated AS (
    SELECT 
        CIK,
        NULL AS TICKER,                    -- Placeholder for ticker symbols
        NAME,
        COUNTRYBA,                         -- Business address country
        STPRBA,                           -- Business address state
        -- Additional address and corporate structure fields
        ROW_NUMBER() OVER (PARTITION BY CIK, NAME ORDER BY CURRENT_TIMESTAMP) AS row_num
    FROM {{source('raw_source', 'sub')}}
)
SELECT * FROM deduplicated WHERE row_num = 1
```

**Dimension Features**:
- **Slowly Changing Dimension**: Type 1 SCD implementation
- **Deduplication Logic**: Handles multiple filings per company
- **Comprehensive Attributes**: Business and mailing addresses
- **Data Lineage**: Built-in audit fields

#### **Financial Statement Facts**

##### **Balance Sheet Facts (`FACT_BALANCE_SHEET.sql`)**
```sql
{{ config(
    unique_key=['company_sk', 'tag_sk', 'adsh', 'filing_date', 'fiscal_year', 'fiscal_period', 'period_end_date', 'qtrs', 'uom', 'value', 'line_item', 'report']
) }}

WITH staged_results AS (
    SELECT 
        a.adsh, a.tag, a.version, a.value, a.qtrs, a.uom,
        b.line, b.report,
        c.cik, c.name, c.fy, c.fp, c.filed, c.period  
    FROM {{source('raw_source', 'num')}} a 
    LEFT JOIN {{source('raw_source', 'pre')}} b 
    JOIN {{source('raw_source', 'sub')}} c 
        ON a.adsh = b.adsh AND a.tag = b.tag 
        AND a.version = b.version AND a.adsh = c.adsh
    WHERE b.stmt = 'BS'                    -- Balance Sheet filter
)
SELECT DISTINCT
    dc.company_sk,                         -- Foreign key to company dimension
    dc.name AS COMPANY_NAME,
    dt.tag_sk,                            -- Foreign key to tag dimension
    s.filed AS filing_date,
    s.fy AS fiscal_year,
    s.fp AS fiscal_period,
    s.period AS period_end_date,
    s.value,                              -- Financial statement value
    s.line AS line_item,                  -- Presentation line number
    CURRENT_TIMESTAMP AS CREATED_DT
FROM staged_results s
JOIN {{ref('DIM_COMPANY')}} dc ON s.cik = dc.cik AND s.name = dc.name
JOIN {{ref('DIM_TAG')}} dt ON s.tag = dt.tag AND s.version = dt.version
```

**Fact Table Design**:
- **Star Schema**: Optimized for analytical queries
- **Financial Metrics**: Standardized GAAP and IFRS measures
- **Time Dimensions**: Multiple temporal perspectives
- **Data Integrity**: Comprehensive referential integrity

### dbt Testing Framework

#### **Data Quality Tests (`models/dw/dw_schema.yml`)**
```yaml
models:
  - name: FACT_BALANCE_SHEET
    columns:
      - name: company_sk
        tests:
          - relationships:
              to: ref('DIM_COMPANY')
              field: company_sk
          - not_null
      - name: tag_sk
        tests:
          - relationships:
              to: ref('DIM_TAG')
              field: tag_sk
      - name: fct_bs_sk
        tests:
          - not_null 
          - unique
```

**Testing Coverage**:
- **Referential Integrity**: Foreign key constraint validation
- **Data Completeness**: NOT NULL constraint testing
- **Uniqueness**: Primary key validation
- **Custom Tests**: Business rule validation

## ðŸš€ Apache Airflow Pipeline Architecture

### Pipeline Overview

The system implements five specialized DAGs for comprehensive data orchestration:

#### **1. Data Discovery Pipeline (`get_datalinks.py`)**
```python
def get_links(web_link = "https://www.sec.gov/data-research/sec-markets-data/financial-statement-data-sets"):
    app = FirecrawlApp(api_key='fc-0d5722bd706743c0900235dc38d4651e')
    response = app.scrape_url(url=web_link, params={
        'formats': [ 'links' ],
        'onlyMainContent': False,
        'includeTags': [ 'tr', 'a', 'href' ],
        'excludeTags': [ 'headers' ]
    })
    
    zip_links = [links for links in response['links'] if links.endswith('.zip')]
    
    # Structure data by year/quarter
    sec_data = {}
    for link in zip_links:
        year_qtr = link.split('/')[-1].split('.zip')[0]
        year, qtr = year_qtr[:4], year_qtr[-1]
        if year not in sec_data:
            sec_data[year] = {}
        sec_data[year][qtr] = link
```

**FireCrawl Integration Features**:
- **Intelligent Scraping**: AI-powered content extraction
- **Link Discovery**: Automated ZIP file detection
- **Structured Output**: Hierarchical year/quarter organization
- **S3 Metadata Storage**: JSON-based link inventory

#### **2. Data Extraction Pipeline (`scrape_upload_dag.py`)**
```python
def store_data_to_s3(**context):
    year = context['params'].get('year','2024')
    qtr = context['params'].get('qtr','4')
    
    # SEC compliance headers
    headers = {
        'User-Agent': 'MIT bigdata@gmail.com',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*',
        'Host': 'www.sec.gov',
        'Connection': 'keep-alive'
    }
    
    # Stream download and extract
    with requests.get(zip_link, stream=True, headers=headers) as zip_response:      
        zip_buffer = BytesIO()
        for chunk in zip_response.iter_content(chunk_size=8 * 1024 * 1024):
            zip_buffer.write(chunk)
            
        with zipfile.ZipFile(zip_buffer, 'r') as zip:
            for file_name in zip.namelist():
                with zip.open(file_name) as file_obj:
                    cleaned_data = file_obj.read().replace(b"\\", b"")
                    s3_key = f"sec_data/{year}/{qtr}/{file_name.split('.')[0]}.tsv"
                    s3_client.upload_fileobj(cleaned_file_obj, bucket_name, s3_key)
```

**Advanced Extraction Features**:
- **Streaming Processing**: Memory-efficient large file handling
- **SEC Compliance**: Proper headers and rate limiting
- **Data Cleaning**: Backslash removal for SQL compatibility
- **Parallel Uploads**: Multi-threaded S3 operations

#### **3. dbt Integration Pipelines**

##### **Raw Data Pipeline (`dbt_raw_dag.py`)**
```python
dbt_raw = BashOperator(
    task_id="dbt_curl_command",
    bash_command="""
    curl -X POST "https://bn544.us1.dbt.com/api/v2/accounts/70471823424708/jobs/70471823425377/run/" \
      -H "Authorization: Token dbtu_aSKjCT4DBp7NVMX7ZIOj9Meosndn7W8Y2pD3K--X3a-v_pArxA" \
      -H "Content-Type: application/json" \
      -d '{
          "cause": "Triggered via API",
          "steps_override": [
          "dbt run --select raw --vars \\"{\\"year\\": \\"{{ params.year }}\\", \\"qtr\\": \\"{{ params.qtr }}\\"}\\""
          ]
      }'
    """,
    params={'year': '{{ task_instance.xcom_pull(key="year") }}'},
    trigger_rule='none_failed_min_one_success'    
)
```

**dbt Cloud Integration**:
- **API-Driven Execution**: Programmatic dbt run triggering
- **Dynamic Variables**: Year/quarter parameter passing
- **Selective Execution**: Model-specific run commands
- **Dependency Management**: Trigger rule configuration

### Pipeline Dependencies

```
get_datalinks â†’ check_if_file_exists â†’ [upload_data_to_s3, dbt_raw]
                                    â†’ upload_data_to_s3 â†’ dbt_raw
                                    â†’ dbt_raw â†’ dbt_json
                                    â†’ dbt_raw â†’ dbt_denormalize
```

## ðŸ”§ Environment Setup & Deployment

### Prerequisites
```bash
# Core Requirements
- Python 3.10+
- Docker & Docker Compose
- AWS Account (S3 access)
- Snowflake Account
- dbt Cloud Account
```

### Environment Configuration
```env
# AWS Configuration
ACCESS_KEY=your_aws_access_key
SECRET_ACCESS_KEY=your_aws_secret_key
REGION=us-east-2
S3_BUCKET_NAME=your_s3_bucket

# Snowflake Configuration
SF_USER=your_snowflake_user
SF_PASSWORD=your_snowflake_password
SF_ACCOUNT=your_snowflake_account

# Airflow Configuration
AIRFLOW_USERNAME=airflow
AIRFLOW_PASSWORD=airflow

# FireCrawl API
FIRECRAWL_API_KEY=fc-your-api-key
```

### Deployment Options

#### **Option 1: Docker Orchestration (Recommended)**
```bash
# Deploy Airflow cluster
cd airflow
docker-compose up -d

# Verify services
docker-compose ps

# Access Airflow UI
open http://localhost:8080
```

#### **Option 2: Local Development**
```bash
# Install dependencies
pip install -r requirements.txt

# Start FastAPI backend
cd backend/fastApi
uvicorn main:app --reload --port 8000

# Launch Streamlit frontend
cd frontend
streamlit run streamlit-app.py
```

### Snowflake Configuration

#### **External Stage Setup**
```sql
-- Create file format for TSV processing
CREATE FILE FORMAT IF NOT EXISTS sec_tsv_format 
TYPE = 'CSV' 
FIELD_DELIMITER = '\t'
SKIP_HEADER = 1
FIELD_OPTIONALLY_ENCLOSED_BY = '"';

-- Create external stage pointing to S3
CREATE STAGE IF NOT EXISTS sec_s3_stage
URL = 's3://your-bucket/sec_data/'  
FILE_FORMAT = sec_tsv_format         
CREDENTIALS = (
    AWS_KEY_ID = 'your_access_key' 
    AWS_SECRET_KEY = 'your_secret_key'
);

-- Create external table for dbt integration
CREATE EXTERNAL TABLE sec_ext_table
LOCATION = @sec_s3_stage
FILE_FORMAT = sec_tsv_format;
```

## ðŸ“Š API Integration & Usage

### FastAPI Backend Architecture

#### **Dynamic Query Endpoint**
```python
@app.get("/user_query/{query}/{year}/{qtr}/{schema}")
def user_query(query: str, year: str, qtr: str, schema: str):
    # Data availability check
    check_sf = {
        "raw": f"select nvl(count(adsh),0) data from raw.sub where year(filed)={year} and quarter(filed)={qtr};",
        "json": f"select nvl(count(data_hash),0) as data from json.sub where year(data:FILED::date)={year} and quarter(data:FILED::date)={qtr};",
        "dw": f"select count(fct_bs_sk) as data from dw.fact_balance_sheet where year(filing_date)={year} and quarter(filing_date)={qtr};"
    }
    
    if data_count > 0:
        # Execute user query with limit
        cur.execute(f"USE SCHEMA {schema}")
        cur.execute(query + " LIMIT 100")
        return {"status": "success", "data": df.to_dict(orient='records')}
    else:
        # Trigger appropriate Airflow DAG
        return trigger_dag(task_dict[schema], year, qtr)
```

**API Features**:
- **Dynamic Schema Selection**: Raw, JSON, or DW layer access
- **Data Validation**: Automatic data availability checking
- **Pipeline Triggering**: On-demand DAG execution
- **Result Limiting**: Performance-optimized query execution

#### **Airflow Integration**
```python
def trigger_dag(dag_id: str, year, qtr):
    url = f"{AIRFLOW_BASE_URL}/dags/{dag_id}/dagRuns"   
    response = requests.post(
        url,
        auth=(USERNAME, PASSWORD),
        json={"conf": {"year": year, "qtr": qtr}}
    )
    return {"message": f"DAG {dag_id} triggered successfully!"}
```

## ðŸš¦ Data Storage Approaches Comparison

| Approach | Use Case | Advantages | Query Pattern |
|----------|----------|------------|---------------|
| **Raw Layer** | Data Lineage & Auditing | â€¢ Complete data fidelity<br>â€¢ Full audit trail<br>â€¢ Regulatory compliance | `SELECT * FROM raw.sub WHERE cik = 320193` |
| **JSON Layer** | Flexible Analytics | â€¢ Schema evolution<br>â€¢ Nested data queries<br>â€¢ NoSQL-style access | `SELECT data:NAME FROM json.sub WHERE data:CIK = 320193` |
| **Dimensional Layer** | Business Intelligence | â€¢ Optimized for BI tools<br>â€¢ Pre-joined relationships<br>â€¢ Aggregation friendly | `SELECT company_name, SUM(value) FROM fact_balance_sheet f JOIN dim_company d ON f.company_sk = d.company_sk` |

## ðŸ” Advanced Query Examples

### Raw Layer Analysis
```sql
-- Companies by fiscal year and quarter
SELECT DISTINCT name, fy, fp 
FROM raw.sub 
WHERE fy = 2024 AND fp = 'Q3'
ORDER BY name;
```

### JSON Layer Analysis  
```sql
-- Flexible company search with JSON path queries
SELECT 
    data:NAME::STRING as company_name,
    data:CIK::INT as central_index_key,
    data:FILED::DATE as filing_date
FROM json.sub 
WHERE data:STPRBA::STRING = 'CA'  -- California companies
  AND YEAR(data:FILED::DATE) = 2024;
```

### Dimensional Layer Analysis
```sql
-- Financial statement analysis across multiple periods
SELECT 
    dc.name,
    fbs.fiscal_year,
    fbs.fiscal_period,
    dt.tag,
    SUM(fbs.value) as total_value
FROM dw.fact_balance_sheet fbs
JOIN dw.dim_company dc ON fbs.company_sk = dc.company_sk
JOIN dw.dim_tag dt ON fbs.tag_sk = dt.tag_sk
WHERE dc.name LIKE '%Apple%'
  AND dt.tag IN ('Assets', 'Liabilities')
GROUP BY 1,2,3,4
ORDER BY fiscal_year, fiscal_period;
```

## ðŸ§ª Data Quality & Testing Framework

### dbt Testing Strategy

#### **Schema Tests (`dw_schema.yml`)**
```yaml
models:
  - name: FACT_BALANCE_SHEET
    columns:
      - name: fiscal_year
        tests:
          - not_null
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 1990
              max_value: 2030
      - name: company_sk
        tests:
          - relationships:
              to: ref('DIM_COMPANY')
              field: company_sk
```

#### **Custom Tests (`tests/stg_sub.sql`)**
```sql
-- Validate filing dates are not in the future
select * from {{source("raw_source", "sub")}} 
where filed > current_date()
```

## ðŸ“ˆ Performance Optimization

### Query Optimization
- **Clustering Keys**: Time-based clustering on filing dates
- **Materialized Views**: Pre-computed aggregations
- **Incremental Models**: Efficient processing of new data only
- **Partitioning**: Year/quarter-based table partitioning

### Cost Management
- **Auto-suspend**: Automatic warehouse suspension
- **Result Caching**: Query result cache utilization  
- **Compute Scaling**: Right-sized warehouse selection
- **Storage Optimization**: Time travel retention policies

## ðŸŽ¯ Business Value & Use Cases

### Financial Analysis
- **Fundamental Analysis**: Multi-period financial metric comparison
- **Industry Analysis**: Sector-based performance benchmarking
- **Regulatory Compliance**: SEC reporting validation and monitoring
- **Investment Research**: Data-driven investment decision support

### Data Engineering Showcase
- **Modern ELT**: Demonstrates cloud-native data processing patterns
- **dbt Best Practices**: Advanced transformation modeling techniques
- **Pipeline Orchestration**: Production-ready workflow management
- **Multi-layer Architecture**: Scalable data warehouse design

This comprehensive SEC financial data pipeline represents a production-grade solution for financial data processing, demonstrating advanced data engineering practices with sophisticated dbt transformations, automated pipeline orchestration, and flexible data access patterns.
